<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Transformer Circuits Thread</title>
    <link>https://transformer-circuits.pub/feed_transformer_circuits.xml</link>
    <description>Latest interpretability research from Anthropic</description>
    <atom:link href="https://transformer-circuits.pub/feed_transformer_circuits.xml" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <image>
      <url>https://transformer-circuits.pub/interp.png</url>
      <title>Transformer Circuits Thread</title>
      <link>https://transformer-circuits.pub/feed_transformer_circuits.xml</link>
    </image>
    <language>en</language>
    <lastBuildDate>Mon, 29 Dec 2025 10:27:35 +0000</lastBuildDate>
    <item>
      <title>Circuits Updates â November 2025</title>
      <link>https://transformer-circuits.pub/2025/november-update/index.html</link>
      <description>A short update on harm pressure.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/november-update/index.html</guid>
      <category>Note</category>
      <pubDate>Sat, 01 Nov 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Emergent Introspective Awareness in Large Language Models</title>
      <link>https://transformer-circuits.pub/2025/introspection/index.html</link>
      <description>Lindsey, 2025 - We find evidence that language models can introspect on their internal states.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/introspection/index.html</guid>
      <category>Paper</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â October 2025</title>
      <link>https://transformer-circuits.pub/2025/october-update/index.html</link>
      <description>Small updates on visual features and dictionary initialization.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/october-update/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>When Models Manipulate Manifolds: The Geometry of a Counting Task</title>
      <link>https://transformer-circuits.pub/2025/linebreaks/index.html</link>
      <description>Gurnee et al., 2025 - We find geometric structure underlying the mechanisms of a fundamental language model behavior.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/linebreaks/index.html</guid>
      <category>Paper</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â September 2025</title>
      <link>https://transformer-circuits.pub/2025/september-update/index.html</link>
      <description>A small update on features and in-context learning.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/september-update/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â August 2025</title>
      <link>https://transformer-circuits.pub/2025/august-update/index.html</link>
      <description>A small update: How does a persona modify the assistantâs response?</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/august-update/index.html</guid>
      <category>Note</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Toy Model of Mechanistic (Un)Faithfulness</title>
      <link>https://transformer-circuits.pub/2025/faithfulness-toy-model/index.html</link>
      <description>When transcoders go awry.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/faithfulness-toy-model/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Tracing Attention Computation Through Feature Interactions</title>
      <link>https://transformer-circuits.pub/2025/attention-qk/index.html</link>
      <description>Kamath et al., 2025 - We describe and apply a method to explain attention patterns in terms of
                    feature interactions, and integrate this information into attribution graphs.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attention-qk/index.html</guid>
      <category>Paper</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Toy Model of Interference Weights</title>
      <link>https://transformer-circuits.pub/2025/interference-weights/index.html</link>
      <description>Unpacking "interference weights" in some more depth.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/interference-weights/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Sparse mixtures of linear transforms</title>
      <link>https://transformer-circuits.pub/2025/bulk-update/index.html</link>
      <description>We investigate sparse mixture of linear transforms (MOLT), a new approach to transcoders.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/bulk-update/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â July 2025</title>
      <link>https://transformer-circuits.pub/2025/july-update/index.html</link>
      <description>A collection of small updates: revisiting A Mathematical Framework and applications of
                    interpretability to biology.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/july-update/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Automated Auditing</title>
      <link>https://alignment.anthropic.com/2025/automated-auditing/</link>
      <description>A note on using agents to perform automated alignment audits, including using interpretability
                    tools.</description>
      <guid isPermaLink="false">https://alignment.anthropic.com/2025/automated-auditing/</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â April 2025</title>
      <link>https://transformer-circuits.pub/2025/april-update/index.html</link>
      <description>A collection of small updates: jailbreaks, dense features, and spinning up on interpretability.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/april-update/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Progress on Attention</title>
      <link>https://transformer-circuits.pub/2025/attention-update/index.html</link>
      <description>An update on our progress studying attention.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attention-update/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>On the Biology of a Large Language Model</title>
      <link>https://transformer-circuits.pub/2025/attribution-graphs/biology.html</link>
      <description>Lindsey et al., 2025 - We investigate the internal mechanisms used by Claude 3.5 Haiku â Anthropic's lightweight
                    production model â in a variety of contexts.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attribution-graphs/biology.html</guid>
      <category>Paper</category>
      <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuit Tracing: Revealing Computational Graphs in Language Models</title>
      <link>https://transformer-circuits.pub/2025/attribution-graphs/methods.html</link>
      <description>Ameisen et al., 2025 - We describe an approach to tracing the "step-by-step" computation involved when a model responds
                    to a single prompt.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attribution-graphs/methods.html</guid>
      <category>Paper</category>
      <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Insights on Crosscoder Model Diffing</title>
      <link>https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html</link>
      <description>A preliminary note on using crosscoders to diff models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html</guid>
      <category>Note</category>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â January 2025</title>
      <link>https://transformer-circuits.pub/2025/january-update/index.html</link>
      <description>A collection of small updates: dictionary learning optimization techniques.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/january-update/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Stage-Wise Model Diffing</title>
      <link>https://transformer-circuits.pub/2024/model-diffing/index.html</link>
      <description>A preliminary note on model diffing through dictionary fine-tuning.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/model-diffing/index.html</guid>
      <category>Note</category>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Sparse Crosscoders for Cross-Layer Features and Model Diffing</title>
      <link>https://transformer-circuits.pub/2024/crosscoders/index.html</link>
      <description>A preliminary note on a way to get consistent features across layers, and even models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/crosscoders/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Using Dictionary Learning Features as Classifiers</title>
      <link>https://transformer-circuits.pub/2024/features-as-classifiers/index.html</link>
      <description>A preliminary note comparing feature-based and raw-activation based harmfulness classifiers.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/features-as-classifiers/index.html</guid>
      <category>Note</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â September 2024</title>
      <link>https://transformer-circuits.pub/2024/september-update/index.html</link>
      <description>A collection of small updates: investigating successor heads, oversampling data in SAEs.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/september-update/index.html</guid>
      <category>Note</category>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â August 2024</title>
      <link>https://transformer-circuits.pub/2024/august-update/index.html</link>
      <description>A collection of small updates: interpretability evals, reproducing self-explanation.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/august-update/index.html</guid>
      <category>Note</category>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â July 2024</title>
      <link>https://transformer-circuits.pub/2024/july-update/index.html</link>
      <description>A collection of small updates: five hurdles, linear representations, dark matter, pivot tables,
                    feature sensitivity.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/july-update/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â June 2024</title>
      <link>https://transformer-circuits.pub/2024/june-update/index.html</link>
      <description>A collection of small updates: topk and gated SAE investigation.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/june-update/index.html</guid>
      <category>Note</category>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</title>
      <link>https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</link>
      <description>Templeton et al., 2024 - Using a sparse autoencoder, we extract a large number of interpretable features from Claude 3
                    Sonnet. Some appear to be safety-relevant.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</guid>
      <category>Paper</category>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â April 2024</title>
      <link>https://transformer-circuits.pub/2024/april-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/april-update/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â March 2024</title>
      <link>https://transformer-circuits.pub/2024/march-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/march-update/index.html</guid>
      <category>Note</category>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Reflections on Qualitative Research</title>
      <link>https://transformer-circuits.pub/2024/qualitative-essay/index.html</link>
      <description>Some opinionated thoughts on why interpretability research may have
                    qualitative aspects be more central than we're used to in other fields.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/qualitative-essay/index.html</guid>
      <category>Note</category>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â February 2024</title>
      <link>https://transformer-circuits.pub/2024/feb-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/feb-update/index.html</guid>
      <category>Note</category>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â January 2024</title>
      <link>https://transformer-circuits.pub/2024/jan-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/jan-update/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</title>
      <link>https://transformer-circuits.pub/2023/monosemantic-features/index.html</link>
      <description>Bricken et al., 2023 - Using a sparse autoencoder, we extract a large number of interpretable features from a one-layer
                    transformer.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/monosemantic-features/index.html</guid>
      <category>Paper</category>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â July 2023</title>
      <link>https://transformer-circuits.pub/2023/july-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/july-update/index.html</guid>
      <category>Note</category>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates â May 2023</title>
      <link>https://transformer-circuits.pub/2023/may-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/may-update/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Interpretability Dreams</title>
      <link>https://transformer-circuits.pub/2023/interpretability-dreams/index.html</link>
      <description>Our present research aims to create a foundation for mechanistic
                    interpretability research. In doing so, it's important to keep sight of what we're trying to lay
                    the foundations for.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/interpretability-dreams/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Distributed Representations: Composition &amp; Superposition</title>
      <link>https://transformer-circuits.pub/2023/superposition-composition/index.html</link>
      <description>An informal note on how "distributed representations" might be understood
                    as two different, competing strategies â "composition" and "superposition" â with quite
                    different properties.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/superposition-composition/index.html</guid>
      <category>Note</category>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Privileged Bases in the Transformer Residual Stream</title>
      <link>https://transformer-circuits.pub/2023/privileged-basis/index.html</link>
      <description>Our mathematical theories of the Transformer architecture suggest that
                    individual coordinates in the residual stream should have no special
                    significance, but recent work has shown that this observation is false in practice.
                    We investigate this phenomenon and provisionally conclude that the per-dimension normalizers in
                    the Adam optimizer are to blame for the effect.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/privileged-basis/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Superposition, Memorization, and Double Descent</title>
      <link>https://transformer-circuits.pub/2023/toy-double-descent/index.html</link>
      <description>Henighan et al., 2023 - We have little mechanistic understanding of how deep learning
                    models overfit to their training data, despite it being a
                    central problem. Here we extend our previous work on toy models
                    to shed light on how models generalize beyond their training
                    data.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/toy-double-descent/index.html</guid>
      <category>Paper</category>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Toy Models of Superposition</title>
      <link>https://transformer-circuits.pub/2022/toy_model/index.html</link>
      <description>Elhage et al., 2022 - Neural networks often seem to pack many unrelated concepts into a single
                    neuron - a puzzling phenomenon known as 'polysemanticity'. In our latest interpretability work,
                    we build toy models where the origins and dynamics of polysemanticity can be fully understood.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/toy_model/index.html</guid>
      <category>Paper</category>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Softmax Linear Units</title>
      <link>https://transformer-circuits.pub/2022/solu/index.html</link>
      <description>An alternative activation function increases the fraction of neurons which
                    appear to correspond to human-understandable concepts.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/solu/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases</title>
      <link>https://transformer-circuits.pub/2022/mech-interp-essay/index.html</link>
      <description>An informal note on intuitions related to mechanistic interpretability.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/mech-interp-essay/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>In-Context Learning and Induction Heads</title>
      <link>https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</link>
      <description>Olsson et al., 2022 - An exploration of the hypothesis that induction heads are the primary
                    mechanism behind in-context learning. We also report the existence of a previously unknown phase
                    change in transformers language models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</guid>
      <category>Paper</category>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Mathematical Framework for Transformer Circuits</title>
      <link>https://transformer-circuits.pub/2021/framework/index.html</link>
      <description>Elhage et al., 2021 - Our early mathematical framework for reverse engineering models,
                    demonstrated by reverse engineering small toy models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/framework/index.html</guid>
      <category>Paper</category>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Exercises</title>
      <link>https://transformer-circuits.pub/2021/exercises/index.html</link>
      <description>Some exercises we've developed to improve our understanding of how neural
                    networks implement algorithms at the parameter level.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/exercises/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Videos</title>
      <link>https://transformer-circuits.pub/2021/videos/index.html</link>
      <description>Very rough informal talks as we search for a way to reverse engineering
                    transformers.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/videos/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>PySvelte</title>
      <link>https://github.com/anthropics/PySvelte</link>
      <description>One approach to bridging Python and web-based interactive diagrams for
                    interpretability research.</description>
      <guid isPermaLink="false">https://github.com/anthropics/PySvelte</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Garcon</title>
      <link>https://transformer-circuits.pub/2021/garcon/index.html</link>
      <description>A description of our tooling for doing interpretability on large models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/garcon/index.html</guid>
      <category>Note</category>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Original Distill Circuits Thread</title>
      <link>https://distill.pub/2020/circuits/</link>
      <description>Our exploration of Transformers builds heavily on the original Circuits
                    thread on Distill.</description>
      <guid isPermaLink="false">https://distill.pub/2020/circuits/</guid>
      <category>Paper</category>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
